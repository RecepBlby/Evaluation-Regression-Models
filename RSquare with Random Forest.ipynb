{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Upload the dataset\n",
    "diamonds = pd.read_csv('diamonds.csv')\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features in the text format, and we need to encode them to the numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "diamonds = diamonds.drop(['Unnamed: 0'], axis = 1)\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Convert the variables to numerical\n",
    "for i in range(3):\n",
    "    new = le.fit_transform(diamonds[categorical_features[i]])\n",
    "    diamonds[categorical_features[i]] = new\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Random Forest algorithm is that it doesn't require data scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and target\n",
    "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z', 'clarity', 'cut', 'color']]\n",
    "y = diamonds[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model and making prediction\n",
    "\n",
    "# Make necessary imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)\n",
    "\n",
    "# Train the model\n",
    "regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)\n",
    "regr.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pre-trained model and can estimate it by making the prediction of the diamonds prices and comparing them with the real prices from test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make prediction\n",
    "predictions = regr.predict(X_test)\n",
    "\n",
    "result = X_test\n",
    "result['price'] = y_test\n",
    "result['prediction'] = predictions.tolist()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define x axis\n",
    "x_axis = X_test.carat\n",
    "\n",
    "# Build scatterplot\n",
    "plt.scatter(x_axis, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')\n",
    "plt.scatter(x_axis, predictions, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(color = '#D3D3D3', linestyle = 'solid')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted prices (red scatters) coincide well with the real ones (blue scatters), especially in the region of small carat values. But to estimate our model more precisely, we will look at Mean absolute error (MAE), Mean squared error (MSE), and R-squared scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test.values.ravel(), predictions)\n",
    "\n",
    "# Mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test.values.ravel(), predictions)\n",
    "\n",
    "# R-squared scores\n",
    "r2 = r2_score(y_test.values.ravel(), predictions)\n",
    "\n",
    "# Print metrics\n",
    "print('Mean Absolute Error:', round(mae, 2))\n",
    "print('Mean Squared Error:', round(mse, 2))\n",
    "print('R-squared scores:', round(r2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared value is rather good, but the errors are high. To improve this situation, we should tune the hyperparameters of the algorithm a little.\n",
    "\n",
    "Special tools from sklearn library can help us perform the tuning faster and more effective. One of such tools is GridSearchCV method which will obtain the best parameters for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the parameters\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Find the best parameters for the model\n",
    "parameters = {\n",
    "    'max_depth': [70, 80, 90, 100],\n",
    "    'n_estimators': [900, 1000, 1100]\n",
    "}\n",
    "gridforest = GridSearchCV(regr, parameters, cv = 3, n_jobs = -1, verbose = 1)\n",
    "gridforest.fit(X_train, y_train)\n",
    "gridforest.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change paramaters and erors decreased and R-squared scores increased which means that the algorithm with the tuned hyperparameters has higher prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and visualizing variables importance\n",
    "Some of features influence the price greater than the others. If we define the most important features, we will be able to use only those in calculations and in such way improve the performance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features list\n",
    "characteristics = X.columns\n",
    "\n",
    "# Get the variables importances, sort them, and print the result\n",
    "importances = list(regr.feature_importances_)\n",
    "characteristics_importances = [(characteristic, round(importance, 2)) for characteristic, importance in zip(characteristics, importances)]\n",
    "characteristics_importances = sorted(characteristics_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in characteristics_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the variables importances\n",
    "plt.bar(characteristics, importances, orientation = 'vertical')\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Variable')\n",
    "plt.grid(axis = 'y', color = '#D3D3D3', linestyle = 'solid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only four features have a great influence on the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
